{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About dataset\n",
    "### Kaggle Competition | Titanic Machine Learning from Disaster\n",
    "\n",
    ">The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    ">One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    ">In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    ">This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n",
    "\n",
    "From the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you are using Google Colab\n",
    "#!wget https://raw.githubusercontent.com/PrzemekSekula/DeepLearningClasses1/master/Titanic/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataframe\n",
    "Let's read our data in using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\") \n",
    "print (df.shape)\n",
    "print (df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description\n",
    "\n",
    "The files we read in the previous screen are available on the data page for the Titanic competition on Kaggle. That page also has a data dictionary, which explains the various columns that make up the data set. Below are the descriptions contained in that data dictionary:\n",
    "\n",
    "- PassengerID - A column added by Kaggle to identify each row and make submissions easier\n",
    "- Survived - Whether the passenger survived or not and the value we are predicting (0=No, 1=Yes)\n",
    "- Pclass - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd)\n",
    "- Sex - The passenger's sex\n",
    "- Age - The passenger's age in years\n",
    "- SibSp - The number of siblings or spouses the passenger had aboard the Titanic\n",
    "- Parch - The number of parents or children the passenger had aboard the Titanic\n",
    "- Ticket - The passenger's ticket number\n",
    "- Fare - The fare the passenger paid\n",
    "- Cabin - The passenger's cabin number\n",
    "- Embarked - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the missing values for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take care of missing values:\n",
    "The features `cabin` has many missing values and so canâ€™t add much value to our analysis. To handle this we will drop them from the dataframe to preserve the integrity of our dataset. It is also a good idea do delete the name of the passanger and the number of ticket - this data also cannot probably improve the quality of our predictions. Let's do it with pandas.\n",
    "\n",
    "    df = df.drop(['ticket','cabin', 'Name'], axis=1) \n",
    "\n",
    "The next step is to do something with other missing values. Please note that we have 2 missing values in `Embarked` and 177 in `Age` columns. There are several possible solutions, one of them is simply delete all the rows with missing values. You can do it using the following pandas function.\n",
    "   \n",
    "    df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Ticket','Cabin', 'Name'], axis=1)\n",
    "# Remove NaN values\n",
    "df = df.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 -  Let's take a look at our data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 a) Data analysis\n",
    "Let's check:\n",
    "- how many people survived, and how many don't *(hint - google for pandas value_counts)*\n",
    "- what is the Passanger Class (`Pclass` column) distribution of our data\n",
    "- what is the age distribution of our data *(hint - use pandas histogram)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Pclass.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b -  Correlation analysis \n",
    "Prepare the correlation analysis of the data. In analysis you should display the information about correlations between each pair of variables.\n",
    "\n",
    "*Hint: You may google for `pandas correlation matrix heatmap` to find the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = df.select_dtypes(include=np.number).corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)\n",
    "\n",
    "def magnify():\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", \"7pt\")]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"12pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '200px'),\n",
    "                        ('font-size', '12pt')])\n",
    "]\n",
    "\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_caption(\"Hover to magify\")\\\n",
    "    .format(\"{:.2f}\")\\\n",
    "    .set_table_styles(magnify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Select features and labels\n",
    "Create the new dataframes/series:\n",
    "- `X` - with the features\n",
    "- `y` - with the labels\n",
    "\n",
    "These dataframes should be ready to use train_test_split, and then to perform machine learning.\n",
    "\n",
    "**Note: This task will be probably done iteratively - you may get back to this task every time you want to improve your results **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Survived\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Pclass', 'Fare']] # Doesn't work well\n",
    "X = df[['Pclass', 'Fare', 'Sex', 'SibSp', 'Parch']] # Works well\n",
    "X = df[['Pclass', 'Fare', 'Sex']] # Also works well\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 One-hot Encoding\n",
    "Perform one-hot encoding if necessary. \n",
    "\n",
    "*Hint: Use `pd.get_dummies`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=['Pclass', 'Sex']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Train test split\n",
    "<img src=\"./images/TrainTestSplit.jpg\">\n",
    "Split the data into training and testing sets. Use the following parameters:\n",
    "- Size of testing set = 25% of entire datasize\n",
    "- Your training / testing sets should contain aproximately the same ratio of survived (use `stratify`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify = y, random_state = 1)\n",
    "\n",
    "print ('X train shape:', X_train.shape)\n",
    "print ('X test shape:', X_test.shape)\n",
    "print ('y train shape:', y_train.shape)\n",
    "print ('y test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Train your model\n",
    "Create and train your model. You may use any model you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print ('Model trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Test your model\n",
    "Test your model. If necessary change something and train your model again.\n",
    "Your goal is to prepare the model with the accuracy >= 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train set accuracy:\", model.score(X_train, y_train))\n",
    "print (\"Test set accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test different models now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(min_samples_leaf = 1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print ('Train accuracy:', model.score(X_train, y_train))\n",
    "print ('Test accuracy:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "print ('Train accuracy:', model.score(X_train, y_train))\n",
    "print ('Test accuracy:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
